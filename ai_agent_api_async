import json
from ai_agent import graph, State
from fastapi import FastAPI, Request, Response
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from langchain_core.messages import (
    convert_to_openai_messages,
    message_chunk_to_message
)
import logging

logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s: %(levelname)s: %(message)s",
)

logger = logging.getLogger(__name__)


app = FastAPI(
    title="API for openwebui-pipeline",
    description="API for openwebui-pipeline",
    )

@app.post("/openwebui-pipelines/api")
async def main(inputs: State):
    response = await graph.ainvoke(inputs)
    logging.info(response)
    return response

@app.post("/openwebui-pipelines/api/stream")
async def stream(inputs: State):
    async def event_stream():
        try:
            logging.debug(f"Request: {inputs}")
            response_text = ""
            async for event in graph.astream(input=inputs, stream_mode="messages"):
                # get first element of tuple
                message = message_chunk_to_message(event[0])
                response_text += convert_to_openai_messages(message)['content']               
                yield convert_to_openai_messages(message)['content']
        except Exception as e:
            logging.error(f"An error occurred: {e}")

        logging.debug(f"Response: {response_text}")

    return StreamingResponse(event_stream(), media_type="application/json")

@app.get("/")
async def read_root():
    return {"AI Agent API": "Running"}

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8000)
